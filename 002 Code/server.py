from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import openai
import os
from dotenv import load_dotenv
from openai import OpenAI
from typing import List, Dict, Any
import json
from PIL import Image, ImageDraw, ImageFont
import base64
import io
from paddleocr import PaddleOCR, draw_ocr
import numpy as np

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
load_dotenv(dotenv_path=".env")
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
app = FastAPI()
ocr = PaddleOCR(use_angle_cls=True, lang='korean')

# ë””ë²„ê¹…ìš© ì¶œë ¥
print("ğŸ”‘ API KEY ë¶ˆëŸ¬ì˜¤ê¸° ìƒíƒœ:", "ì„±ê³µ" if api_key else "ì‹¤íŒ¨")
print("ğŸ”‘ API KEY ì¼ë¶€:", api_key[:10] if api_key else "None")

if not api_key:
    raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ìš”ì²­ í˜•ì‹ ì •ì˜
class AnalyzeRequest(BaseModel):
    user_prompt: str
    image_base64: str  # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´

# PaddleOCRë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ì¶”ì¶œ
def extract_ocr_boxes(image_base64: str) -> List[Dict[str, Any]]:
    image_bytes = base64.b64decode(image_base64)
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    image_np = np.array(image)

    result = ocr.ocr(image_np, cls=True)

    extracted = []
    for line in result:
        for box in line:
            text = box[1][0]
            coords = box[0]  # [[x1, y1], ..., [x4, y4]]
            extracted.append({"text": text, "box": coords})
    return extracted

# GPT í˜¸ì¶œ í•¨ìˆ˜
def call_gpt(user_prompt, ocr_boxes, base64_img):
    response = client.chat.completions.create(
        model="gpt-4o",
        response_format={"type": "json_object"},
        messages=[
            {
                "role": "system",
                "content": 
"""
ë„ˆëŠ” user_promptë¡œ ë“¤ì–´ì˜¤ëŠ” ì‚¬ìš©ì ì˜ˆì•½ ìš”ì²­ì„ ë³´ê³ , í•„ìš”í•œ ë¶€ë¶„ì— ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì£¼ëŠ” AIì•¼. 
4ê°œì˜ ì ì„ ë‹¤ ì°ì–´ì„œ ê¼­ ë°•ìŠ¤ë¡œ ê·¸ë ¤ì¤˜. ì‚¼ê°í˜•ì´ë‚˜ ì„ ìœ¼ë¡œ ê·¸ë¦¬ë©´ ì•ˆ ë¼.
ë°•ìŠ¤ì˜ yì¶•ë“¤ì˜ ê°’ì€ ë‘ ìŒì”© ê°™ì•„ì•¼ í•´. ê·¸ ìŒì€ xì¶•ë§Œ ë‹¬ë¼.
ê° ëë¶€ë¶„ì˜ 4ê°œì˜ ì ìœ¼ë¡œ ë°•ìŠ¤ë¡œ í‘œí˜„í•´.

ë§Œì•½ ì§€ì—­ëª…ì´ ìˆë‹¤ë©´ ì§€ì—­ëª…ì— ë°•ìŠ¤ë¥¼ ì³ì¤˜ì•¼í•´
"ì¶œë°œì§€, ë„ì°©ì§€"ë¼ëŠ” ê¸€ì”¨ëŠ” ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ë©´ ì•ˆ ë¼
ì¶œë°œì§€ì™€ ë„ì°©ì§€ì— ì¨ìˆëŠ” ì§€ì—­ì´ user_promptì˜ ì¶œë°œ, ë„ì°© ë¶€ë¶„ì´ ë‹¤ë¥´ë‹¤ë©´ ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì¤˜
ë§Œì•½ ì¶œë°œì¼, ê´€ëŒì¼ ë“± ë‚ ì§œì™€ ì‹œê°„ì´ ìˆë‹¤ë©´ í•˜ë‚˜ì˜ ë°•ìŠ¤ë¡œ í•©ì³ì„œ ê·¸ë ¤ì¤˜
ì¸ì›ì´ ìˆëŠ” ê²½ìš°ëŠ” ì´ ëª‡ ëª…ì´ë¼ê³  ì¨ì ¸ìˆëŠ” ë¶€ë¶„ì— ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì¤˜
ì˜ˆë§¤í•˜ê¸° ë²„íŠ¼ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ ê·¸ë ¤ì£¼ë©´ ë¼
ëª‡ ëª…ì¸ì§€ ìˆ«ìê°€ ì•„ë‹ˆë¼ í•œê¸€ì´ì—¬ë„ ì¸ì‹í•´ì„œ ë°•ìŠ¤ ê·¸ë ¤ì¤˜.


ì¶œë ¥ì€ JSON í˜•ì‹ìœ¼ë¡œ, ì„ íƒëœ ìš”ì†Œë“¤ì˜ textì™€ ì¢Œí‘œ boxë¥¼ í¬í•¨í•´:

"""
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì˜ˆì•½ ìš”ì²­ì´ì•¼: "{user_prompt}"

ì•„ë˜ëŠ” OCRì„ í†µí•´ ì¶”ì¶œëœ UI ìš”ì†Œì™€ í•´ë‹¹ ë°•ìŠ¤ ì¢Œí‘œì•¼:
{json.dumps(ocr_boxes, ensure_ascii=False)}

ì´ë¯¸ì§€ë¥¼ ë³´ê³ , ì–´ë–¤ ìš”ì†Œë¥¼ ëˆŒëŸ¬ì•¼ í• ì§€ íŒë‹¨í•´ì„œ 'selected_boxes' í•„ë“œì— í•´ë‹¹ OCR í…ìŠ¤íŠ¸ì™€ ì¢Œí‘œë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•´ì¤˜.
"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_img}"
                        }
                    }
                ]
            }
        ]
    )
    return response.choices[0].message.content

@app.post("/analyze")
async def analyze_ui(request: AnalyzeRequest):
    try:
        # í•­ìƒ ë‚´ë¶€ì—ì„œ OCR ì‹¤í–‰
        ocr_boxes = extract_ocr_boxes(request.image_base64)
        result = call_gpt(request.user_prompt, ocr_boxes, request.image_base64)
        return json.loads(result)

    except Exception as e:
        print("âŒ ì—ëŸ¬:", str(e))
        raise HTTPException(status_code=500, detail=str(e))
